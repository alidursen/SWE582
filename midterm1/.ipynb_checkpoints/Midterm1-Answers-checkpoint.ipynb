{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Answer to Q1:\n",
    "\"Consider how $\\alpha$ and $\\beta$ are used in our regression models.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, we can consider $\\alpha$ as <i>ratio</i> of our next step. By \"next step\" we mean the vector $dE + \\beta \\cdot step_{previous}$, and by ratio, a coefficient which is applied to this vector. So, for example, an $\\alpha$ of 0.01 would mean we're going to go from $P$ to $P + (0.01dE + 0.01\\beta step_{previous})$. Further, $\\beta$ stands for ratio of our previous behaviour with respect to our derivative. That can be important in cases where we don't want derivative to completely dominate/dictate how our location will change, but will be impacted by our previous decisions.<br>\n",
    "Particular cases are as follows:<br>\n",
    "<ul><li>$\\alpha$ too small, $\\beta$ too small: Since $\\beta$ is too small, it'll be as if it's nonexistent, ie. $dE$ will be the dominant factor. However, since $\\alpha$ too is too small, there will be little-to-no change at all, and this kind of an algorithm will take many steps to converge to target point.\n",
    "<li>$\\alpha$ too large, $\\beta$ too small: Once again, a $\\beta$ too small means our steps are dominated by $dE$. However, in this case $\\alpha$ being <i>too large</i> implies that it is large beyond point of usefulness. In other words, this carries the risk of our sequence being divergent, or that it \"blows up\".\n",
    "<li>$\\alpha$ too small, $\\beta$ large: $\\beta$ being <i>large</i> shows that its existence contributes a significant amount. Further, some intuitive dimension analysis suggests $\\alpha \\beta$ is only <i>small</i> (since $small \\cdot large = normal$, and $verysmall \\cdot large = small$) and though our step sizes are decreasing, this will be slower than the case where both were small. This situation can be expected to converge to our target point relatively quickly.\n",
    "<li>$\\alpha$ too large, $\\beta$ small: Similar to $2^{nd}$ case, a very large $\\alpha$ is one that is beyond usefulness. Therefore, though $\\beta$ can be significant, in the end we could have a divergent sequence.\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Answer to Q2:\n",
    "\"Use the logistic regression model to determine if input is in class 1; compare with Naive Bayesian method.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# MOST IDEAS/CODE COPIED FROM PREVIOUS IRIS EXERCISE, SEE:\n",
    "# https://github.com/SWE582-Fall-2017-Bogazici/naive-bayes-alidursen/blob/master/Iris%20-%20A%20Continuous%20Exercise.ipynb\n",
    "\n",
    "import numpy\n",
    "import pandas\n",
    "import math\n",
    "\n",
    "IrisData = pandas.read_csv('bezdekIris.csv', header=-1).set_index([4])\n",
    "X = IrisData.as_matrix()\n",
    "StatsByClass = numpy.dstack((numpy.array([IrisData.loc['Iris-setosa'].mean().as_matrix(),\n",
    "                                          IrisData.loc['Iris-versicolor'].mean().as_matrix(),\n",
    "                                          IrisData.loc['Iris-virginica'].mean().as_matrix()]),\n",
    "                             numpy.array([IrisData.loc['Iris-setosa'].std().as_matrix(),\n",
    "                                          IrisData.loc['Iris-versicolor'].std().as_matrix(),\n",
    "                                          IrisData.loc['Iris-virginica'].std().as_matrix()])))\n",
    "\n",
    "def NormalProb(point, mean=0., std=1., interval=0.01):\n",
    "    xlower = ((point-mean)-interval*0.5)/std\n",
    "    xupper = ((point-mean)+interval*0.5)/std\n",
    "    return (math.erf(xupper/math.sqrt(2.))-math.erf(xlower/math.sqrt(2.)))/2.\n",
    "\n",
    "def Sigmoid(x):\n",
    "    return 1/(1+numpy.exp(-x))\n",
    "\n",
    "def LogSumExp(a,b):\n",
    "    return a + numpy.log(1+numpy.exp(b-a))\n",
    "\n",
    "def SetosaBayes(sl,sw,pl,pw):\n",
    "    # As usual, will accept (up to) 4 floats and will return a matrix of 2 entries:\n",
    "    # 1st possibility of being 'Setosa', 2nd of not being.\n",
    "    # But honestly, since we want to easily compare two methods, it is easier to\n",
    "    # return a boolean, True if P('Setosa')>=0.5, False otherwise\n",
    "    param = [sl, sw, pl, pw]\n",
    "    pS = numpy.zeros((4,3))\n",
    "    for i in range(4):\n",
    "        for k in range(3):\n",
    "            pS[i,k] = NormalProb(point=param[i], mean=StatsByClass[k,i,0], std=StatsByClass[k,i,1])\n",
    "    p = numpy.ones(3)\n",
    "    for i in range(4):\n",
    "        p = p*pS[i]\n",
    "    return ( p[0]*2. >= numpy.sum(p) )\n",
    "\n",
    "# To get i'th row/data point from data: (returns a numpy array, because why not?)\n",
    "# def Row(i):\n",
    "#     return IrisData.iloc[i:i+1,:].as_matrix()\n",
    "# No need to define a function that never gets used.\n",
    "\n",
    "y = numpy.array([int(u=='Iris-setosa') for u in IrisData.index.values])\n",
    "w = numpy.zeros(4)\n",
    "p = numpy.zeros(4)\n",
    "yTX = numpy.dot(y.T, X)\n",
    "# Obtaining a suitable w\n",
    "alpha = 0.01\n",
    "beta = 0.8\n",
    "trials = 1000\n",
    "for epoch in range(trials):\n",
    "    L, dL = (numpy.dot(yTX, w)-numpy.sum(LogSumExp(0,numpy.dot(X,w))),\n",
    "            numpy.dot(X.T,y-Sigmoid(numpy.dot(X,w))))\n",
    "    p = dL + beta*p\n",
    "    w = w + alpha*p\n",
    "    #if (epoch+1)%100==0:\n",
    "    #    print('Step', epoch,':', L)\n",
    "\n",
    "def SetosaLogistic(sl,sw,pl,pw):\n",
    "    # As discussed in SetosaBayes method, will return a boolean.\n",
    "    # Since I don't fully know if we can use logistic method on attribute subsets,\n",
    "    # I'll mandate entry of all 4 attributes. This, in turn, will necessitate the same for SetosaBayes.\n",
    "    \n",
    "    # Originally we had Sigmoid(V) >= 0.5 here, but by the nature of Sigmoid it's equivalent to V => 0\n",
    "    # Of course, this is due to my lax condition: \"Better than 50% is good enough.\" A stricter condition\n",
    "    # would rightfully use Sigmoid function itself.\n",
    "    return (numpy.inner(numpy.array([sl,sw,pl,pw]),w) >= 0.)\n",
    "\n",
    "def MethodComparison(*e):\n",
    "    \"\"\"4 floats needed.\"\"\"\n",
    "    return (SetosaBayes(*e) == SetosaLogistic(*e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.755, 0.55)"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from random import *\n",
    "means = numpy.array([5.84,3.05,3.76,1.20])*2\n",
    "trial = 200\n",
    "overalltrial = 200\n",
    "overallc = []\n",
    "ocM, ocm = 0,1\n",
    "for i in range(overalltrial):\n",
    "    c,t = 0,0\n",
    "    for j in range(trial):\n",
    "        l = numpy.array([random(),random(),random(),random()])*means\n",
    "        r = MethodComparison(*l)\n",
    "        if r:\n",
    "            c += 1\n",
    "        t = c/float(trial)\n",
    "    if t>ocM:\n",
    "        ocM = t\n",
    "    elif t<ocm:\n",
    "        ocm = t\n",
    "    overallc.append(t)\n",
    "ocM,ocm\n",
    "# This way of finding max/min is faster than using max(),min() functions respectively.\n",
    "# Note that, this is still not very fast, there are currently 40,000 trials going on, after all."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numbers above can be changed according to one's heart's desire, but I believe it's fair to suggest Naive Bayesian and Logistic model align between 50 to 75% of the time. Given that Bayesian model is inherently flawed, this does not necessarily imply a failure of logistic model or our implementation of it.<br>\n",
    "Yet, upon further study, it's seen that we did not include column of uniform 1's that is generally added for flexibility in calculations. That might have given us a different $w$ than would be otherwise available."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2 - PyTorch Addendum\n",
    "#### Fatal errors:\n",
    "Somehow my torch is incompatible with my system. Whenever I try to run some method (even rather trivial ones!) I'd have the kernel shutdown and restart.<br>However, I also had some theoretical problems as well. As commented below, I checked example pyTorchExample.ipynb. There we introduce an error function, <br>\n",
    "<b>EuclidianLoss = torch.nn.MSELoss(size_average=True)</b><br>\n",
    "which we later call with<br>\n",
    "<b>EuclidianLoss(f(Variable(x)), Variable(y))</b><br>\n",
    "What is going on here? Does torch.nn.MSELoss() returns a <i>function</i> that accepts 2 parameters? If that is the case, how can we update that to our situation? For example, I discovered there is a torch.nn.Softplus() function that can stand in for our LogSumExp function. However, what does it return? A number? A function like torch.nn.MSELoss() does?<br><br>\n",
    "Some further problems: Do we need column of pure 1's to account for bias terms or not? How to transform a tensor into our data set? All in all, not only my technical problems, but also my unfamiliarity with torch led me to present an incomplete answer to 2nd question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.399999976158142"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Before we begin, it's helpful to see these two:\n",
    "# https://github.com/torch/demos/blob/master/linear-regression/example-linear-regression.lua\n",
    "# https://github.com/atcemgil/notes/blob/master/pyTorchExample.ipynb\n",
    "\n",
    "import torch\n",
    "X_ = torch.Tensor(numpy.insert(X,4,1,axis=1))\n",
    "# Note that this step introduces some floating error.\n",
    "# For example, X_[1][2] returns 1.399999976158142 where it should've been just 1.4\n",
    "# However, this is _very_ insignificant. Just not the _same_ dataset, is all.\n",
    "\n",
    "# torch provides its own LogSumExp(0,w): Softplus()\n",
    "LSE = torch.nn.Softplus()\n",
    "\n",
    "# Learning rate:\n",
    "eta = 0.01\n",
    "\n",
    "for epoch in range(10):\n",
    "    E = "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
